{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NowGo-LLM API: Practical Usage Examples\n",
    "\n",
    "This notebook demonstrates how to interact with the NowGo-LLM backend API, focusing on the `/v1/chat/interactive` endpoint to showcase agent orchestration based on context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, ensure your NowGo-LLM backend server is running. You can start it from the `/home/ubuntu/NowGo-LLM/backend` directory with:\n",
    "\n",
    "```bash\n",
    "source venv/bin/activate\n",
    "uvicorn app.main:app --reload --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "Also, make sure you have an `.env` file in the `/home/ubuntu/NowGo-LLM/` directory with your `OPENAI_API_KEY`.\n",
    "\n",
    "Install the `requests` library if you haven't already:\n",
    "\n",
    "```bash\n",
    "pip install requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Define the base URL for the API\n",
    "BASE_URL = \"http://localhost:8000\"\n",
    "\n",
    "def pretty_print_json(data):\n",
    "    print(json.dumps(data, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Checking API Health\n",
    "\n",
    "Let's start by checking if the API is running correctly using the `/health` endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = requests.get(f\"{BASE_URL}/health\")\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors (4xx or 5xx)\n",
    "    print(\"API Health Check:\")\n",
    "    pretty_print_json(response.json())\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error connecting to API: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interacting with the Chat Endpoint (`/v1/chat/interactive`)\n",
    "\n",
    "This endpoint allows you to send a user prompt along with contextual information (user ID, company ID, module accessed) to get a response from the orchestrated LLM agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_chat_interactive(user_id: str, company_id: str, prompt: str, module_accessed: str = None, current_interaction_data: dict = None):\n",
    "    endpoint = f\"{BASE_URL}/v1/chat/interactive\"\n",
    "    payload = {\n",
    "        \"user_id\": user_id,\n",
    "        \"company_id\": company_id,\n",
    "        \"prompt\": prompt,\n",
    "        \"module_accessed\": module_accessed,\n",
    "        \"current_interaction_data\": current_interaction_data or {}\n",
    "    }\n",
    "    \n",
    "    print(f\"--- Sending Request to /v1/chat/interactive ---\")\n",
    "    print(\"Payload:\")\n",
    "    pretty_print_json(payload)\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(endpoint, json=payload)\n",
    "        response.raise_for_status()\n",
    "        print(\"\nResponse from API:\")\n",
    "        pretty_print_json(response.json())\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            try:\n",
    "                print(\"Error details:\")\n",
    "                pretty_print_json(e.response.json())\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Raw error response: {e.response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: User as a Manager in a Technology Company (Strategy Consultant Persona)\n",
    "\n",
    "User `user123` is a Manager in `comp456` (Technology sector). Let's ask a strategic question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_ex1 = \"user123\"\n",
    "company_id_ex1 = \"comp456\"\n",
    "prompt_ex1 = \"What are the key growth strategies for a SaaS company in the current market?\"\n",
    "module_ex1 = \"strategy_dashboard\"\n",
    "\n",
    "response_ex1 = call_chat_interactive(user_id_ex1, company_id_ex1, prompt_ex1, module_accessed=module_ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: User as Legal Counsel (Legal Expert Persona)\n",
    "\n",
    "User `user789` is Legal Counsel in `comp001` (Manufacturing sector). Let's ask a compliance-related question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_ex2 = \"user789\"\n",
    "company_id_ex2 = \"comp001\"\n",
    "prompt_ex2 = \"What are the main considerations for data privacy when launching a new product in Europe?\"\n",
    "module_ex2 = \"legal_compliance_checker\"\n",
    "\n",
    "response_ex2 = call_chat_interactive(user_id_ex2, company_id_ex2, prompt_ex2, module_accessed=module_ex2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Generic User (Default Persona or Fallback)\n",
    "\n",
    "User `unknown_user` in `unknown_comp`. The system should handle this gracefully, possibly using a default persona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_ex3 = \"unknown_user\"\n",
    "company_id_ex3 = \"unknown_comp\"\n",
    "prompt_ex3 = \"Can you explain the concept of a Large Language Model in simple terms?\"\n",
    "\n",
    "response_ex3 = call_chat_interactive(user_id_ex3, company_id_ex3, prompt_ex3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Requesting Content Creation (Growth Writer Persona)\n",
    "\n",
    "Let's assume the orchestrator can infer the need for a Growth Writer based on the module or prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_ex4 = \"user123\" # Manager from Tech company
",
    "company_id_ex4 = \"comp456\"\n",
    "prompt_ex4 = \"Draft a short blog post intro about the benefits of AI for small businesses.\"\n",
    "module_ex4 = \"content_creation_tool\" # This module might trigger the Growth Writer
",
    "\n",
    "response_ex4 = call_chat_interactive(user_id_ex4, company_id_ex4, prompt_ex4, module_accessed=module_ex4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further Exploration\n",
    "\n",
    "- Try different `user_id` and `company_id` combinations to see how the agent's persona might change (based on the mock data in `ContextManager`).\n",
    "- Experiment with different `module_accessed` values.\n",
    "- Observe the `interaction_history` being built up by making sequential calls for the same user/company.\n",
    "- Modify the `ContextManager` in the backend to include more diverse user/company profiles and test the agent's adaptability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

